My second go at the netflix prize. (Feb. 25, 2007) 
http://www.netflixprize.com/rules
http://www.netflixprize.com/faq

? http://pyflix.python-hosting.com/
? http://docs.python.org/lib/module-mmap.html

Goals:
    use python (possibly IronPython) and/or C
        (both for the callenge,
         and to try to break the problem into smaller processes)
    use simon's methon (in some way)
    reach at least the level of the Progress Prize 2007 - RMSE: 0.9419
    

    
Simon's "Try This at Home" post (summary):
    http://interstice.com/journals/Simon/20061211.html
    http://en.wikipedia.org/wiki/Singular_value_decomposition
    http://en.wikipedia.org/wiki/Ridge_regression
    
    Given 100M Ratings (1-5) of 17K Movies by 500K Users as (User,Movie,Rating):
        Find Rating for (User,Movie,?)
    Visualize as a matrix of users across the top and movies down the side.
    Each cell in the matrix is a rating of 1-5, or blank.
    This matrix would have about 8.5 billion entries, and is only 1/85 filled.
    (Dates of rating events are also given, but not used (currently).)
    Understanding is compression (and sometimes, vice versa).
    Singular value decomposition - in this usage:
        define a set of features (action, length, Will Smith, etc...)
        describe a movie's quantity of each feature
        describe each user's preference for each feature
        user's rating is the sum of (preferency * movie's quantity)
    Our goal is to now find the feature, quantities (movieValue), and preferences (userValue) 
        that minimize error between our predictions and reality.
    We do this by following the derivative of the error.
        userValue[user] += lrate * err * movieValue[movie];
        movieValue[movie] += lrate * err * userValue[user];
        //lrate = learning rate (suggested: 0.001)
    or
        void train(int user, int movie, real rating)
        {
        	real err = lrate * (rating - predictRating(movie, user));
        
        	userValue[user] += err * movieValue[movie];
        	movieValue[movie] += err * userValue[user];
        }
    Initialize quantities and preferences to 0.1.
    Train on one feature, then move on to the next, and the next...
    Start by getting the average rating for each movie,
        and the average offset for each user from the movie's average rating.
    Baseline prediction is then: averageRating[movie] + averageOffset[user].]
    Note: some movies (and users) haven't been rated (or rated) enough to determine a good average.
    In these cases, we should blend the known average with the average of all ratings.
    BetterMean = (GlobalAverage*K + sum(ObservedRatings)) / (K + count(ObservedRatings))
        where K = 25.
    We have similar problems for updating our values, so let's change our update code to:
        userValue[user] += lrate * (err * movieValue[movie] - K * userValue[user]);
        movieValue[movie] += lrate * (err * userValue[user] - K * movieValue[movie]);
        //so, the farther we are from zero, the less we change the value
        //Vincent liked K=0.02
    We have a problem with predictions outside the range of 1-5: they're always wrong.
    Clipping is better than noting,
    but some sigmoid function that makes low ratings stronger than high ratings works better.
    Beware of over fitting.  120 epochs per feature is about right.
    
Approach:
    pre-process the downloaded files and convert them into nice binary files
        probe set [movieId(2), custId(3), rating(1)] = 6 B * 1400000 =~ 8.4 MB
        the following with and without probe (for submitting, and for testing):
        movie index [id(2), rowId(4), ratingCount(3), avgRating(2)] = 11B * 17770 = 195,470 B
        movie data [custId(3), rating(1)] = 4 * 100000000 =~ 400 MB
        cust index [custId(3), rowId(4), ratingCount(2), avgOffset(2)] = 11B * 480000 - 5,280,000 B
        cust data [movieId(2), rating(1)] = 3 * 100000000 =~ 300 MB
    build feature files for movies and customers (m_0, c_0, m_1, ...)  
        movie feature [id(2), score(4)] = 6 * 17770 = 106,620 B
        cust feature [custId(3), score(4)] = 7 * 480000 = 3,360,000 B
        //feature 0 is based on averages and should produce a reasonable RMSE (<1 ?)
        //to build feature set N, we need to have movie and customer info in memory + features 0 to N-1
    run probe
        use the probe set and feature(s) to find the RMSE
    generate predictions
        use the qualifying set to generate a prediction file to submit        
        

    
Required Fast Functions:
    getMovieRatingCount(movieId) -> count (0 to 480189)
    getMovieRatingAvg(movieId) -> avg (1.0 to 5.0)
    getCustRatingCount(custId) -> count (0 to 17770)
    getCustRatingOffset(custId) -> offset (-5 to +5)
    getRating(movieId, custId) -> rating
    nextProbe() -> (movieId, custId, rating)
Secondary functions?
    custIdToPos(custId)
    custPosToId(custPos)
    predict(movieId, custId) -> rating
    predict(featureId, movieId, custId) -> rating
To maximize speed:
    1) store everything needed in memory (with room to spare)
    2) little or no searching, only direct seeks where possible
    3) no strings, only numbers and math    
     
    
